<configuration>
  <property>
    <name>trafodion_config_template</name>
    <value>### Basic settings ###
export MONITOR_COMM_PORT={{monitor_comm_port}}
export TRAF_AGENT={{traf_agent}}
export TRAF_HOME={{default_traf_home}}
export TRAF_ABSPATH={{traf_abspath}}
export TRAF_USER={{traf_user}}
export TRAF_VERSION={{traf_version}}
export TRAF_VAR={{traf_var}}
export TRAF_CONF=/etc/trafodion/conf
export REST_CONF_DIR=$TRAF_CONF/rest
export DCS_CONF_DIR=$TRAF_CONF/dcs

export TRAF_LOG={{traf_log}}
export CORE_PATTERN={{core_loc}}
export EXPORTER_NODES={{exporter_list}}
# for compatibility
export MY_SQROOT=$TRAF_HOME
export JAVA_HOME={{java_home}}
# only used in sqgen to detect workstation/cluster
export node_count={{node_count}}
export HADOOP_TYPE={{hadoop_type}}
export ZOOKEEPER_NODES={{zk_nodes}}
export ZOOKEEPER_PORT={{zk_port}}
export CLUSTERNAME={{clustername}}
# used in sqenvcom.sh
export PARCEL_PATH={{parcel_path}}
export MGBLTY_INSTALL_DIR=$TRAF_HOME/mgblty
export DBMGR_INSTALL_DIR=$TRAF_HOME/dbmgr
export PATH=$PATH:$MGBLTY_INSTALL_DIR/jython2.7.0/bin
export HIVESERVER2_URL={{hiveserver2_url}}
export HIVESERVER2_SSL="false"
export DB_ADMIN_USER={{db_admin_user}}
export DB_ROOT_USER={{db_root_user}}

### security settings ###
export SECURE_HADOOP={{secure_hadoop}}
export TRAFODION_AUTHENTICATION_TYPE={{TRAFODION_AUTHENTICATION_TYPE}}
export TRAFODION_PASSWORD_CHECK_SYNTAX={{TRAFODION_PASSWORD_CHECK_SYNTAX}}
export SENTRY_SECURITY_FOR_HIVE={{sentry_for_hive}}
export SENTRY_SECURITY_GROUP_MODE={{hadoop_group_mapping}}
export HIVE_PRINCIPAL={{hive_principal}}

### HA settings ###
export ENABLE_HA={{enable_ha}}
export DCS_MASTER_FLOATING_IP={{dcs_floating_ip}}
export KEEPALIVED={{dcs_ha_keepalived}}

### cgroup settings ###
export ESGYN_CG_CPU={{esgyn_cg_cpu}}
export ESGYN_CG_CPUACCT={{esgyn_cg_cpuacct}}
export ESGYN_CG_MEM={{esgyn_cg_mem}}
export ESGYN_CGP_CPU={{esgyn_cgp_cpu}}
export ESGYN_CGP_CPUACCT={{esgyn_cgp_cpuacct}}
export ESGYN_CGP_MEM={{esgyn_cgp_mem}}

### Apache Hadoop distro settings ###
export HADOOP_PREFIX={{hadoop_home}}
export HBASE_HOME={{hbase_home}}
export HIVE_HOME={{hive_home}}
export PATH=$PATH:$HADOOP_PREFIX/bin:$HADOOP_PREFIX/sbin:$HBASE_HOME/bin

### configs for elasticity ###
export HBASE_USER={{hbase_user}}
export HDFS_USER={{hdfs_user}}
export HBASE_XML_FILE={{hbase_xml_file}}
export HBASE_LIB_PATH={{hbase_lib_path}}
export DCS_CNT_PER_NODE={{dcs_cnt_per_node}}
export DCS_MASTER_NODES={{dcs_master_nodes}}
export SCRATCH_LOCS={{scratch_locs}}
export DISTRO={{distro}}

### For multi instance support ###
export TRAF_CLUSTER_NAME="{{cluster_name}}"
export TRAF_INSTANCE_NAME="{{traf_instance_name}}"
export TRAF_CLUSTER_ID={{traf_cluster_id}}
export TRAF_INSTANCE_ID={{traf_instance_id}}
export TRAF_ROOT_ZNODE=/{{traf_user}}

# CDH/HDP cluster name
export CLUSTER_NAME="{{cluster_name}}"
    </value>
  </property>
  <property>
    <name>ldap_config_template</name>
    <value>
SECTION: Defaults
  DefaultSectionName: {{ldap_domain}}
  RefreshTime: 1800
  TLS_CACERTFilename: {{ldap_certpath}}

SECTION: {{ldap_domain}}
  # one name: value pair for each host.
  # LdapHostname:
{{ldap_hosts}}

  # Default is port 389, change if using 636 or any other port
  LdapPort: {{ldap_port}}

  # Must specify one or more unique identifiers, one name: value pair for each
  # UniqueIdentifier:
{{ldap_identifiers}}

  # If the configured LDAP server requires a username and password to
  # to perform name lookup, provide those here.
  LDAPSearchDN: {{ldap_user}}
  LDAPSearchPwd: {{ldap_pwd}}

  # If configured LDAP server requires TLS(2) or SSL (1), update this value
  LDAPSSL: {{ldap_encrypt}}

  # Default timeout values in seconds
  #LDAPNetworkTimeout: 30
  #LDAPTimeout: 30
  #LDAPTimeLimit: 30
  #RetryCount: 5
  #RetryDelay: 2
  #PreserveConnection: No
  #ExcludeBadHosts: Yes
  #MaxExcludeListSize: 3

  LDAPSearchGroupBase: {{ldap_srch_grp_base}}
  LDAPSearchGroupObjectClass: {{ldap_srch_grp_obj_class}}
  LDAPSearchGroupMemberAttr: {{ldap_srch_grp_mem_attr}}
  LDAPSearchGroupNameAttr: {{ldap_srch_grp_name_attr}}

#SECTION: (name)
#  LdapHostname: (host)
#  UniqueIdentifier: (cn=...)
#  ...
    </value>
  </property>
  <property>
    <name>trafodion_sudoers_template</name>
    <value>
## Allow trafodion id to run commands needed for backup and restore
{{traf_user}} ALL =({{hbase_user}}) NOPASSWD: {{hbase_home}}/bin/hbase, /usr/bin/kill
{{traf_user}} ALL =({{hdfs_user}}) NOPASSWD: /usr/bin/kill

## Allow trafodion id to run commands needed for snmp and ipmitool
{{traf_user}} ALL=(ALL) SETENV:NOPASSWD: {{default_traf_home}}/sql/scripts/snmp_trap_start
{{traf_user}} ALL=(ALL) SETENV:NOPASSWD: {{default_traf_home}}/sql/scripts/snmp_trap_watch
{{traf_user}} ALL=(ALL) SETENV:NOPASSWD: {{default_traf_home}}/sql/scripts/snmp_trap_stop
{{traf_user}} ALL=(ALL) SETENV:NOPASSWD: {{default_traf_home}}/sql/scripts/ipmi_map_gen_node

## Trafodion Floating IP commands
Cmnd_Alias IP = /sbin/ip
Cmnd_Alias ARP = /sbin/arping

## Allow Trafodion id to run commands needed to configure floating IP
{{traf_user}} ALL = NOPASSWD: IP, ARP

## Do not require tty for trafodion user. No password prompts needed
Defaults:{{traf_user}} !requiretty
    </value>
  </property>
  <property>
    <name>cdh_trafodion_sudoers_template</name>
    <value>
## Allow trafodion id to run commands needed for backup and restore
{{traf_user}} ALL=({{hbase_user}}) NOPASSWD: {{hbase_home}}/bin/hbase, /usr/bin/kill

## Allow trafodion id to run commands needed for managing hadoop services
{{traf_user}} ALL=({{hbase_user}}) NOPASSWD: {{hbase_home}}/bin/hbase-daemon.sh, {{hbase_home}}/bin/flush_all_tables, {{hadoop_home}}/sbin/hadoop-daemon.sh, {{zk_home}}/bin/zkServer.sh, {{hive_home}}/bin/hive, {{hadoop_home}}/sbin/yarn-daemon.sh, {{hadoop_home}}/sbin/mr-jobhistory-daemon.sh, /opt/hadoop/bin/hdfs
{{traf_user}} ALL=(ALL) NOPASSWD: /opt/trafodion/omclient/hadoop-manage.sh

## Allow trafodion id to run commands needed for snmp and ipmitool
{{traf_user}} ALL=(ALL) SETENV:NOPASSWD: {{default_traf_home}}/sql/scripts/snmp_trap_start
{{traf_user}} ALL=(ALL) SETENV:NOPASSWD: {{default_traf_home}}/sql/scripts/snmp_trap_watch
{{traf_user}} ALL=(ALL) SETENV:NOPASSWD: {{default_traf_home}}/sql/scripts/snmp_trap_stop
{{traf_user}} ALL=(ALL) SETENV:NOPASSWD: {{default_traf_home}}/sql/scripts/ipmi_map_gen_node

## Trafodion Floating IP commands
Cmnd_Alias IP = /sbin/ip
Cmnd_Alias ARP = /sbin/arping

## Allow Trafodion id to run commands needed to configure floating IP
{{traf_user}} ALL = NOPASSWD: IP, ARP

## Do not require tty for trafodion user. No password prompts needed
Defaults:{{traf_user}} !requiretty
    </value>
  </property>
  <property>
    <name>hadoop_manage_template</name>
    <value>#!/bin/bash

if [[ $1 == 'ZK' ]]; then
    kill -9 $(ps -ef |grep 'QuorumPeerMain' |grep -v grep |awk '{print $2}') 2>/dev/null
    exit 0
elif [[ $1 == 'HDFS' ]]; then
    mkdir -p /var/run/hdfs-sockets/
    chown hadoop:hadoop /var/run/hdfs-sockets
elif [[ $1 == 'mkdir' ]]; then
    dir=${2//,/ }
    mkdir -p $dir
    chmod 700 $dir
    chown hadoop:hadoop $dir
elif [[ $1 == 'rmdir' ]]; then
    dir=${2//,/ }
    for d in $dir; do
        rm -rf $d/*
    done
elif [[ $1 == 'NM' ]]; then
    kill -9 $(ps -ef |grep 'proc_nodemanager' |grep -v grep |awk '{print $2}') 2>/dev/null
    exit 0
elif [[ $1 == 'RM' ]]; then
    kill -9 $(ps -ef |grep 'proc_resourcemanager' |grep -v grep |awk '{print $2}') 2>/dev/null
    exit 0
elif [[ $1 == 'JHS' ]]; then
    kill -9 $(ps -ef |grep 'proc_historyserver' |grep -v grep |awk '{print $2}') 2>/dev/null
    exit 0
fi</value>
  </property>
  <property>
    <name>trafodion_ulimits_template</name>
    <value>
# Trafodion settings
{{traf_user}}   soft   core unlimited
{{traf_user}}   hard   core unlimited
{{traf_user}}   soft   memlock unlimited
{{traf_user}}   hard   memlock unlimited
{{traf_user}}   soft   nofile 32768
{{traf_user}}   hard   nofile 65536
{{traf_user}}   soft   nproc 100000
{{traf_user}}   hard   nproc 100000
{{traf_user}}   soft   stack 10240
{{traf_user}}   hard   stack 10240
    </value>
  </property>
  <property>
    <name>ulimits_template</name>
    <value>
# {{user}} settings
{{user}}   soft   nofile {{nofile}}
{{user}}   hard   nofile {{nofile}}
{{user}}   soft   nproc {{nproc}}
{{user}}   hard   nproc {{nproc}}
{{user}}   soft   memlock unlimited
{{user}}   hard   memlock unlimited
    </value>
  </property>
  <property>
    <name>ldap_changes_template</name>
    <value>dn: {{ldap_dn}}
changetype: modify
replace: olcSuffix
olcSuffix: dc=esgyn,dc=local

dn: {{ldap_dn}}
changetype: modify
replace: olcRootDN
olcRootDN: cn=Manager,dc=esgyn,dc=local

dn: {{ldap_dn}}
changetype: modify
{{rootpw_mode}}: olcRootPW
olcRootPW: {{ldap_rootpw}}

dn: {{ldap_dn}}
changetype: modify
{{access_mode}}: olcAccess
olcAccess: {0}to attrs=userPassword by self write by dn.base="cn=Manager,dc=esgyn,dc=local" write by anonymous auth by * none
olcAccess: {1}to * by dn.base="cn=Manager,dc=esgyn,dc=local" write by self write by * read</value>
  </property>
  <property>
    <name>ldap_ou_template</name>
    <value>dn: dc=esgyn,dc=local
objectClass: dcObject
objectClass: organization
dc: esgyn
o : esgyn

dn: ou=Users,dc=esgyn,dc=local
objectClass: organizationalUnit
ou: Users</value>
  </property>
  <property>
    <name>ldap_users_template</name>
    <value># db_root
dn: uid=trafodion,ou=Users,dc=esgyn,dc=local
uid: trafodion
ou: Users
sn: db_root
cn: DB_ROOT
objectClass: person
objectClass: organizationalPerson
objectClass: inetOrgPerson
userpassword: {{db_root_pwd}}

# db_admin
dn: uid=admin,ou=Users,dc=esgyn,dc=local
ou: Users
uid: admin
sn: db_admin
cn: DB_ADMIN
objectClass: person
objectClass: organizationalPerson
objectClass: inetOrgPerson
userpassword: {{db_admin_pwd}}</value>
  </property>
  <property>
    <name>ldap_mod_syncprov</name>
    <value>dn: cn=module,cn=config
objectClass: olcModuleList
cn: module
olcModulePath: /usr/lib64/openldap
olcModuleLoad: syncprov.la</value>
  </property>
  <property>
    <name>ldap_syncprov</name>
    <value>dn: olcOverlay=syncprov,{{ldap_dn}}
objectClass: olcOverlayConfig
objectClass: olcSyncProvConfig
olcOverlay: syncprov
olcSpCheckpoint: 100 10
olcSpSessionLog: 100</value>
  </property>
  <property>
    <name>ldap_ha</name>
    <value>dn: cn=config
changetype: modify
replace: olcServerID
olcServerID: {{server_id}}

dn: {{ldap_dn}}
changetype: modify
replace: olcSyncRepl
olcSyncRepl: rid=001
             provider=ldap://{{ldap_provider}}:389
             bindmethod=simple
             binddn="cn=Manager,dc=esgyn,dc=local"
             credentials={{ldap_rootpw}}
             searchbase="dc=esgyn,dc=local"
             filter="(objectClass=*)"
             scope=sub
             schemachecking=off
             attrs="*,+"
             type=refreshAndPersist
             retry="5 5 300 +"
             interval=00:00:01:00
-
{{mirror_mode}}: olcMirrorMode
olcMirrorMode: TRUE
-
{{uuid_mode}}: olcDbIndex
olcDbIndex: entryUUID eq
-
{{csn_mode}}: olcDbIndex
olcDbIndex: entryCSN eq</value>
  </property>
  <property>
    <name>esgyn_exporter_yaml_template</name>
    <value>elasticsearch.url: {{es_url}}
elasticsearch.startup.timeout: 60
elasticsearch.basic.auth: N
elasticsearch.basicauth.username:
elasticsearch.basicauth.password:
grpc.port: 23352
grpc.tls: false
regionserver.info.port: 60030
dbm.ip: {{dbm_ip}}
dbm.port: 30022
mxosrvr_memory_limit_mb: 200</value>
  </property>
  <property>
    <name>om_client_template</name>
    <value>#!/bin/bash
#
# @@@ START COPYRIGHT @@@
#
# (C) Copyright 2019 Esgyn Corporation
#
# @@@ END COPYRIGHT @@@

OM_COMPONENT_COUNTS=3
OM_CLIENT_DIR="/opt/trafodion/omclient"
ESGYN_EXPORTER_BINARY="$OM_CLIENT_DIR/esgyn_exporter/esgyn_exporter"
NODE_EXPORTER_BINARY="$OM_CLIENT_DIR/node_exporter/node_exporter"
ESGYN_EXPORTER_PORT=23300
NODE_EXPORTER_PORT=23301

if [ ! -f ${ESGYN_EXPORTER_BINARY} ]; then
   echo "Esgyn Exporter binary cannot be found."
   exit 1;
fi

usage() {
    prog=`basename $0`
    echo "$prog { start | stop | restart | status }"
    echo "    start   -- start the OptMgmt client components: node_exporter/esgyn_exporter/filebeat"
    echo "    stop    -- stop the OptMgmt client components: node_exporter/esgyn_exporter/filebeat"
    echo "    restart -- restart the OptMgmt client components: node_exporter/esgyn_exporter/filebeat"
    echo "    status  -- display the status of OptMgmt client components: node_exporter/esgyn_exporter/filebeat"
}

om_start() {
    om_status
    rc=$?
    if [[ $rc -ne 0 ]]; then
        echo "$(date +%F_%T): Starting OptMgmt client components ..."
        pdsh -w ${EXPORTER_NODES} "mkdir -p $OM_CLIENT_DIR/logs/filebeat"
        pdsh -w ${EXPORTER_NODES} "$ESGYN_EXPORTER_BINARY --web.listen-address=\":$ESGYN_EXPORTER_PORT\"> $OM_CLIENT_DIR/logs/esgyn_exporter.log 2>&amp;1 &amp;"
        pdsh -w ${EXPORTER_NODES} "$NODE_EXPORTER_BINARY --web.listen-address=\":$NODE_EXPORTER_PORT\" > $OM_CLIENT_DIR/logs/node_exporter.log 2>&amp;1 &amp;"
        pdsh -w ${EXPORTER_NODES} "$OM_CLIENT_DIR/filebeat/filebeat --path.config=$OM_CLIENT_DIR/filebeat/ --path.logs=$OM_CLIENT_DIR/logs/filebeat/ >/dev/null 2>$OM_CLIENT_DIR/logs/filebeat/filebeat-err.log &amp;"
        echo "$(date +%F_%T): Started OptMgmt client components"
    fi
}

om_stop() {
    om_status
    rc=$?
    if [[ $rc -ne 1 ]]; then
        echo "$(date +%F_%T): Stopping OptMgmt client components ..."
        pdsh -w ${EXPORTER_NODES} "ps -u $USER -af |grep -E 'esgyn_exporter|node_exporter'|grep -v grep|awk '{print \$2}'|xargs kill 2>/dev/null"
        pdsh -w ${EXPORTER_NODES} "ps -u $USER -af |grep -E 'filebeat'|grep -v grep|awk '{print \$2}'|xargs kill -9 2>/dev/null"
        echo "$(date +%F_%T): Stopped OptMgmt client components"
    fi
}

om_status() {
    act_om_cnt=`pdsh -w ${EXPORTER_NODES} "ps -u $USER -af |grep -E 'esgyn_exporter|node_exporter|filebeat'|grep -v grep" 2>/dev/null|wc -l`
    let cfg_om_cnt=`echo ${EXPORTER_NODES//,/ } | wc -w`*$OM_COMPONENT_COUNTS
    if [[ $act_om_cnt -eq $cfg_om_cnt ]]; then
        echo "$(date +%F_%T): OptMgmt client components are running"
        return 0
    elif [[ $act_om_cnt -eq 0 ]]; then
        echo "$(date +%F_%T): OptMgmt client components are stopped"
        return 1
    elif [[ $act_om_cnt -le $cfg_om_cnt ]]; then
        echo "$(date +%F_%T): OptMgmt client components are partially running: Configured[$cfg_om_cnt]/Actual[$act_om_cnt]"
        return 2
    else
        echo "$(date +%F_%T): Unknown status, please check the environment"
        return 3
    fi
}

case "$1" in
   start)
      om_start
      ;;
   stop)
      om_stop
      ;;
   restart)
      om_stop
      om_start
      ;;
   status)
      om_status
      ;;
   *)
      usage
      exit 1
esac
exit 0</value>
  </property>
  <property>
    <name>hadoop_topology_template</name>
    <value>#!/bin/bash
if [ -z $HADOOP_CNF_DIR ] ; then
    HADOOP_CNF_DIR=/opt/hadoop/etc/hadoop/
fi
while [ $# -gt 0  ] ; do
    nodeArg=$1
    exec&lt;${HADOOP_CNF_DIR}/hadoop-topology
    result=""
    while read line ; do
        ar=( $line )
        if [ "${ar[0]}" = "$nodeArg"  ]||[ "${ar[1]}" = "$nodeArg"  ]; then
            result="${ar[2]}"
        fi
    done
    shift
    if [ -z "$result"  ] ; then
        echo -n "/default-rack"
    else
        echo -n "$result"
    fi
done</value>
  </property>
</configuration>
