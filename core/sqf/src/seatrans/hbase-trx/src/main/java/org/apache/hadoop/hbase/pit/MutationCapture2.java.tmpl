/**
* @@@ START COPYRIGHT @@@
*
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
*   http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing,
* software distributed under the License is distributed on an
* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
* KIND, either express or implied.  See the License for the
* specific language governing permissions and limitations
* under the License.
*
* @@@ END COPYRIGHT @@@
**/

package org.apache.hadoop.hbase.pit;

import java.io.IOException;
import java.lang.Thread.UncaughtExceptionHandler;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

import java.io.*;
import java.util.HashMap;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;
import java.util.ArrayList;
import java.util.List;
import java.util.Collections;
import java.util.LinkedList;

//import org.springframework.util.ReflectionUtils;

import org.apache.hadoop.hbase.io.hfile.CacheConfig;
import org.apache.hadoop.hbase.io.hfile.HFile;
import org.apache.hadoop.hbase.io.hfile.HFileContext;
import org.apache.hadoop.hbase.io.hfile.HFileWriterV2;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.coprocessor.transactional.generated.TrxRegionProtos.TransactionMutationMsg;

import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.SequenceFile.Writer;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.hdfs.DFSClient;
import org.apache.hadoop.hdfs.DFSInputStream;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.Put;

import org.apache.hadoop.hbase.KeyValue;
import org.apache.hadoop.hbase.Stoppable;
import org.apache.hadoop.hbase.HRegionInfo;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
import org.apache.hadoop.hbase.util.FSUtils;
import org.apache.hadoop.hbase.util.Threads;
import org.apache.hadoop.hbase.protobuf.ProtobufUtil;

import java.lang.management.ManagementFactory;
import java.lang.management.MemoryMXBean;

import org.apache.hadoop.hbase.regionserver.transactional.TrxTransactionState;
import org.apache.hadoop.hbase.regionserver.transactional.TrxTransactionState.WriteAction;

import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutationProto.MutationType;
import org.apache.hadoop.hbase.coprocessor.transactional.server.RSConstants;

#ifdef HDP2.3 HDP2.4 CDH5.5 CDH5.7 APACHE1.2 CDH5.16
import org.apache.hadoop.hbase.ChoreService;
import org.apache.hadoop.hbase.ScheduledChore;
#endif

import org.apache.hadoop.hbase.regionserver.transactional.IdTm;
import org.apache.hadoop.hbase.regionserver.transactional.IdTmId;

import org.apache.hadoop.hbase.client.transactional.STRConfig;
import org.apache.hadoop.hbase.client.transactional.ATRConfig;
import org.apache.hadoop.hbase.client.transactional.PeerInfo;

import org.apache.hadoop.hbase.pit.MutationMeta;
import org.apache.hadoop.hbase.pit.MutationMetaRecord;
import org.apache.hadoop.hbase.pit.SnapshotMeta;
import org.apache.hadoop.hbase.pit.HBaseBinlog;

import java.util.concurrent.TimeUnit;
import java.util.concurrent.locks.ReentrantLock;

import org.apache.hadoop.hbase.pit.HBasePitZK;
import org.apache.hadoop.hbase.coprocessor.transactional.server.RSConstants;

public class MutationCapture2 {

  static final Log LOG = LogFactory.getLog(MutationCapture2.class);
  private static final String COMMITTED_TXNS_KEY = "1_COMMITED_TXNS_KEY";

  private static final String binlogTableName = "TRAF_RSRVD_5:TRAFODION._DTM_.TRAFODION_BINLOG";

  Configuration config;
  public HRegionInfo regionInfo;
  FileSystem fs = null;
  HFileContext context;

  private static MemoryMXBean memoryBean = null;

  ByteArrayOutputStream mutationBuffer;
  HFileWriterV2 mutationWriter = null;
  //SST SequenceFile.Writer mutationWriter = null;  
  long mutationCount;
  long mutationTotalCount;
  long mutationTotalSize;
  long mutationSet;
  long totalPuts = 0;
  long totalDeletes = 0;
  String currentFileKey = "";
  long smallestCommitId = -1;  // use Long.MIN_VALUE or -1 assuming cid from iDtm > 0
  long largestCommitId = -1;  // use Long.MAX_VALUE
  long currentSnapshotId = 0;
  long incrementalSnapshotId = 0;
  int MC2_attribute = 0;
  int MC2_fileFormat = PIT_MC2_HFILE;
  boolean skip_CDC = false;
  boolean needWriterCreate = false;
  boolean needWriterAppend = false;
  boolean needWriterClose = false;
  boolean needMetaUpdate = false;
  boolean pitEnabled = false;
  boolean nonTrxRecordBinlog = false;

  STRConfig pSTRConfig = null;

  Path currentMC2Path;
  private String hdfsRoot;
  private String PITRoot;
  private HBasePitZK pitZK;

  private Object mutationBufferLock = new Object();    // sync enqueue and dequeue ops
  private Object mutationWriterLock = new Object();    // sync file ops
  private Object binlogWriterBufferLock = new Object();    // sync file ops
  private static Object binlogDualAppendLock = new Object();   //sync binlog dual append
  AtomicInteger bufferTxId = new AtomicInteger(1);
  AtomicInteger writerTxId = new AtomicInteger(1);
  AtomicInteger choreTxId = new AtomicInteger(1);
  MutationMeta meta = null;
  SnapshotMeta snapMeta = null;
  SnapshotMetaRecord currentSnapshot = null;

  TransactionMutationMsg.Builder [] mcQueue;
  int [] iPutStat, iDeleteStat;
  int qFront = 0;   // queue index should be accessed under mutationBufferLock
  int qRear = 0;
  int qCount = 0;
  int qSize = DEFAULT_MC_QUEUE_SIZE;

  long currBinlogWid = 0;

  MC2TransactionsChore mc2TransactionsChoreThread;
  private Thread mc2ChoreThread = null;
  public static final int DEFAULT_MC2_CHORE_SERVICE_THREAD_POOL_SIZE=10;
  public static final int DEFAULT_MC2_CHORE_SERVICE_FLUSH_TIMER=2000;
  static int txnMC2ChoreServiceThreadPoolSize = DEFAULT_MC2_CHORE_SERVICE_THREAD_POOL_SIZE; // 10 thread pool overriden by
  Stoppable stoppable = new StoppableImplementation();
  static int flushTimer = DEFAULT_MC2_CHORE_SERVICE_FLUSH_TIMER; // 2 secs overriden by
  public boolean enableStrictBinlogAntiDup = false;
  #ifdef HDP2.3 HDP2.4 CDH5.5 CDH5.7 APACHE1.2 CDH5.16
  public static ChoreService mc2_ChoreService = null;
  #endif

  static int initMC = 0;
  static Object mcMapLock = new Object(); // sync MC2 setup, if MC2 object is shared for RM (region)
  public static HashMap<String, MutationCapture2> mcMap = new HashMap<String, MutationCapture2>();
  
  boolean doCreate = false;
  boolean doAppend = false;
  boolean doClose = false;

  public static final int PIT2_MUTATION_WRITER_CREATE = 31;
  public static final int PIT2_MUTATION_WRITER_APPEND = 32;
  public static final int PIT2_MUTATION_WRITER_CLOSE = 33;
  public static final int PIT2_MUTATION_WRITER_METAUPDATE = 34;
  
  public static final int PIT_MC2_HFILE = 0;  
  public static final int PIT_MC2_SEQUENCEFILE = 1;   
  
  public static final int XDC_NONE        = 0;   // 00001
  public static final int XDC_UP          = 1;   // 00001
  public static final int XDC_DOWN        = 2;   // 00010
  public static final int SYNCHRONIZED    = 4;   // 00100
  public static final int SKIP_CONFLICT   = 8;   // 01000
  public static final int SKIP_REMOTE_CK  = 16;  // 10000
  public static final int INCREMENTALBR  = 32;  // 100000
  public static final int TABLE_ATTR_SET  = 64;  // 1000000
  public static final int SKIP_SDN_CDC  = 128;  // 10000000
  public static final int PIT_ALL  = 256;       // 100000000
  public static final int TIMELINE  = 512;      // 1000000000    

  private int binlogSaltNum = 0;

  //public static final int ASYNC_STREAM

  static final int  DEFAULT_MC_QUEUE_SIZE   = 2048;
  public static final int MAX_MSG_PER_MUTATION_MSG = 20 * 1024; //20M
  static long PIT_MC2_max_mutation_perFILE = 10000; // 10K transaction branch of mutations, threahold in a MC file
  static long PIT_MC2_max_size_peFILE = 128000000; // 128 MB size, threahold in a MC file
  static boolean read_config = false;
  static int init_mc2_queue_size = DEFAULT_MC_QUEUE_SIZE;

  public MutationCapture2 (Configuration conf, FileSystem f, HFileContext cont, HRegionInfo rInfo)  throws IOException{
     
       // setup basic attribute, and schedule the MC2 chore 

      this.config = conf;
      this.pitZK = new HBasePitZK(this.config);
      this.regionInfo = rInfo;
      this.fs = f;
      this.context = cont;
      this.skip_CDC = false;
      hdfsRoot = conf.get("fs.default.name");
      PITRoot = hdfsRoot + "/user/trafodion/PIT/" + regionInfo.getTable().toString().replace(":", "_") + "/cdc/";

      String envEnableStrictBinlogAntiDup = System.getenv("ENABLE_STRICT_BINLOG_ANTI_DUP");
      if (envEnableStrictBinlogAntiDup!= null)
        enableStrictBinlogAntiDup = (Integer.parseInt(envEnableStrictBinlogAntiDup.trim()) == 0) ? false : true;

      if (LOG.isInfoEnabled())
          LOG.info("HBaseBinlog: antidup enableStrictBinlogAntiDup is " + enableStrictBinlogAntiDup);

      if (!read_config) {
          init_mc2_queue_size = this.config.getInt("hbase.regionserver.region.transactional.MC2.queue_size",
                                        DEFAULT_MC_QUEUE_SIZE);
          txnMC2ChoreServiceThreadPoolSize = this.config.getInt("hbase.regionserver.region.transactional.MC2.chore_pool_size",
                                        DEFAULT_MC2_CHORE_SERVICE_THREAD_POOL_SIZE); 
          flushTimer = this.config.getInt("hbase.regionserver.region.transactional.MC2.flush_timer",
                                        DEFAULT_MC2_CHORE_SERVICE_FLUSH_TIMER);
          RSConstants.DISABLE_NEWOBJECT_FOR_MC = this.config.getInt("hbase.regionserver.region.transactional.MC2.disableNewObjectForMC",
                                        RSConstants.DISABLE_NEWOBJECT_FOR_MC);
          if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- static coonfigration settings: queue size " + 
              init_mc2_queue_size + " chore pool size " + txnMC2ChoreServiceThreadPoolSize + 
              " flush timer " + flushTimer + " disable new object " + RSConstants.DISABLE_NEWOBJECT_FOR_MC);
          read_config = true;
      }

      if (LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- mc file creation at mutation repository dir " + PITRoot); 

      qFront = qRear = qCount = 0;
      set_CircularQueueCapacity(init_mc2_queue_size, true);
      
      try {
         pSTRConfig = STRConfig.getInstance(this.config);
      } catch (Exception xe) {
         LOG.error("An ERROR occurred while getting the STR Configuration");
      }

      // Start the chore service
              this.mc2TransactionsChoreThread = new MC2TransactionsChore(rInfo.getEncodedName(),this, flushTimer, stoppable);
#ifdef HDP2.3 HDP2.4 CDH5.5 CDH5.7 APACHE1.2 CDH5.16

              if (this.mc2TransactionsChoreThread != null) {
                  setupChoreService();
                  mc2_ChoreService.scheduleChore(this.mc2TransactionsChoreThread);
              }
#else
              UncaughtExceptionHandler handler = new UncaughtExceptionHandler() {
                  public void uncaughtException(final Thread t, final Throwable e) {
                      LOG.fatal("PIT MC2 -- Tansaction MC Chore uncaughtException: " + t.getName(), e);
                  }
              };
              LOG.warn("PIT MC2 -- Transaction CDC does not support chore service for Pre HDP2.3, CDH 5.5, APACHe1.2 ");
              String n = Thread.currentThread().getName();	
              mc2ChoreThread = new Thread(this.mc2TransactionsChoreThread);
              Threads.setDaemonThreadRunning(mc2ChoreThread, n + ".mcTansactionsChore", handler);
#endif

      if (LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- MutationCapture2 constructor() completes");
      return; 
  
  } 

   public synchronized static MutationCapture2 MC2_getInstance
             (Configuration conf, FileSystem f, HFileContext cont, HRegionInfo rInfo, int attr, int newMC2) throws IOException {

  // based on CDC setting, MC would be for region, table, or RS based
  // find instance from static map based on defined key as string, create one if not found

      MutationCapture2 mc2_instance = null;
      String nameMC2 = null;

      synchronized(mcMapLock) {
         
          if (initMC == 0) { // initialization of MC2 static structure, default attributes, chore service

              // read configuration from hbase-site and overide if necessary

              initMC = 1;
          } // static init block

          // initially, support region-based MC2 (0 ~ MC1)
          if (attr == 0) { // default is region base
              nameMC2 = rInfo.getRegionNameAsString();
          }  
          else if (attr == 1) { // table based
              nameMC2 = rInfo.getTable().getNameAsString();
          }
          else { // round-robin RS base,~ HLOG
              LOG.warn("PIT MC2 -- Transaction CDC does not support RS based -- will change to default as region based CDC "); 
              nameMC2 = rInfo.getRegionNameAsString();
          }

          mc2_instance = mcMap.get(nameMC2);
          if ((mc2_instance == null) && (newMC2 != 0)) {
               if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- Transactional MC2 " + nameMC2  + " is create with attr " + attr + " new " 
                  + newMC2+",map="+mcMap.size());
               mc2_instance = new MutationCapture2(conf, f, cont, rInfo);
               mcMap.put(nameMC2, mc2_instance);
          }

      } // mcMapLock

      return mc2_instance;

  }

#ifdef HDP2.3 HDP2.4 CDH5.5
  private synchronized void setupChoreService() { 
      if (mc2_ChoreService == null) {
          mc2_ChoreService = new ChoreService("Transactional CDC ChoreService", MutationCapture2.txnMC2ChoreServiceThreadPoolSize);
          if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- Transaction MC chore thread pool size setting is " + txnMC2ChoreServiceThreadPoolSize); 
      }
  }
#endif

#ifdef CDH5.7 APACHE1.2 CDH5.16
  private synchronized void setupChoreService() {
      if (mc2_ChoreService == null) {
          mc2_ChoreService = new ChoreService("Transactional CDC ChoreService", MutationCapture2.txnMC2ChoreServiceThreadPoolSize, true);
          if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- Transaction MC chore thread pool size setting is " + txnMC2ChoreServiceThreadPoolSize);
      }
  }
 
#endif
  public static String getMc2TransactionsChoreThreadInfo() {
#ifdef CDH5.16
         
    return mc2_ChoreService==null?"There is No Services":mc2_ChoreService.printChoreServiceDetails();
#else
    return "Only Support Version CDH5.16";
#endif

  }
    public MC2TransactionsChore getMc2TransactionsChoreThread() {
        return mc2TransactionsChoreThread;
   }

/**
 * Simple helper class that just keeps track of whether or not its stopped.
 */
   private static class StoppableImplementation implements Stoppable {
     private volatile boolean stop = false;

     @Override
     public void stop(String why) {
       if (LOG.isInfoEnabled())
           LOG.info("PIT MC2 -- Tansaction Chore Thread has stopped: Reason:" + why);
       this.stop = true;
     }

     @Override
     public boolean isStopped() {
       return this.stop;
     }
  }

  public void MC2_doInstanceOperation(int action, int value) {

  // perform MC2 operations
  //               expand MC queue, set CDC attribute, reset chore for hot table, ...

      this.MC2_attribute = value;

   }

  public int MC2_doGetAttribute() {

  // get MC2 attribute

      return this.MC2_attribute;

   }
   
  public void setSkipCDC(boolean flag) {

       this.skip_CDC = flag;
   }   


  // Transactional Mutation Capturer
   
  public void MC2_doMutationAppend(TrxTransactionState ts, String tName) throws IOException {
      
      // build a transaction mutation for the transactional branch and invoke associated MC
      int iPut = 0;
      int iDelete = 0;
      int bufferSeqId = -1;
      
      if(LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- txnMutationAppend -- Transaction Id " + ts.getTransactionId() +
                                                          " commitId " + ts.getCommitId() + " MC2_attribute " + MC2_attribute + " skip " + skip_CDC);

      if (skip_CDC){
         if(LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- skipping CDC for Transaction Id " + ts.getTransactionId()
               + "; commitId " + ts.getCommitId());
         return;
      }
      
      if (pSTRConfig.getPeerStatus(pSTRConfig.getTrafClusterIdInt(), true).contains(PeerInfo.PEER_ATTRIBUTE_NO_CDC_IBR)) {
         int mClient = ts.getMutationClient();
         if (LOG.isDebugEnabled()) LOG.debug("PIT MC2: mutation append: STRCONFIG NO_CDC_IBR is true, trans id " +  ts.getTransactionId() + 
                        " local cluster " + pSTRConfig.getTrafClusterIdInt() + " CDC attribute " + mClient);
         boolean ibr_mc = ((mClient & INCREMENTALBR) == INCREMENTALBR);
         boolean xdc_catchup =  (((mClient & SYNCHRONIZED) == SYNCHRONIZED)
                                           && ((mClient & XDC_DOWN) == XDC_DOWN)  );
                                           
         if (  (ibr_mc) && (!xdc_catchup)  ) {
            if (LOG.isDebugEnabled()) LOG.debug("PIT MC2: mutation append: bypass IBR CDC due to STRCONFIG NO_CDC_IBR, trans id " +  ts.getTransactionId() + 
                     " CDC attribute " + ts.getMutationClient());
            return;
         }
      }

      try {
  
          TransactionMutationMsg.Builder tmBuilder =  TransactionMutationMsg.newBuilder();
          tmBuilder.setTxId(ts.getTransactionId());
          tmBuilder.setTableName(tName);
          tmBuilder.setStartId(ts.getStartId());
          tmBuilder.setCommitId(ts.getCommitId());
          tmBuilder.setTableCDCAttr(ts.getMutationClient());
          tmBuilder.setTotalNum(ts.getTotalNum());
          tmBuilder.setDdlNum(ts.getDdlNum());
          if (ts.getQueryContext() != null)
             tmBuilder.setClientInfo(ts.getQueryContext());

          // NOTE. since mutation append is no-wait, TLOG & HLOG record must contain commit-id
          // to build correct mutation in case of audit replay or in-doubt txn resolution
          int msgCounter = 0;
          int msgSize = 0;
          long splitSeq = 0L;
          for (WriteAction wa : ts.getWriteOrdering()) {
             byte[] rawValue = null;
             if(msgCounter >= 1000 || msgSize >= MAX_MSG_PER_MUTATION_MSG) {
               synchronized(mutationBufferLock) {
                  tmBuilder.setIsMsgComplete(false);
                  splitSeq++;
                  tmBuilder.setSplitSeq(splitSeq);
                  bufferSeqId = circularBuffer_Attach(tmBuilder, iPut, iDelete, ts.getMutationClient());
               }
               //18371
               //move the binlog append outside of mutationBufferLock lock area
               if (ATRConfig.instance().isATRXDCEnabled() && ( binlogIsMaxProtectionMode() == false || nonTrxRecordBinlog == true))
               {
                 currBinlogWid = binlog_append(tmBuilder, ATRConfig.instance().getSyncMode());
               }
               tmBuilder =  TransactionMutationMsg.newBuilder();
               tmBuilder.setTxId(ts.getTransactionId());
               tmBuilder.setTableName(tName);
               tmBuilder.setStartId(ts.getStartId());
               tmBuilder.setCommitId(ts.getCommitId());
               tmBuilder.setTableCDCAttr(ts.getMutationClient());
               tmBuilder.setTotalNum(ts.getTotalNum());
               //reset counter and continue loop
               iPut = 0;
               iDelete = 0;
               msgCounter = 0;
               msgSize = 0;
             }
             if (wa.getPut() != null) {
                 tmBuilder.addPutOrDel(true);
                 if (RSConstants.DISABLE_NEWOBJECT_FOR_MC > 0) 
                    tmBuilder.addPut(ProtobufUtil.toMutation(MutationType.PUT, wa.getPut()));
                 else
                    tmBuilder.addPut(ProtobufUtil.toMutation(MutationType.PUT, new Put(wa.getPut())));
                 iPut++;
             }
             else {
                if (! wa.getIgnoreDelete()){  // Don't apply delete if we subsequently have a put for the same key
                   tmBuilder.addPutOrDel(false);
				   if (RSConstants.DISABLE_NEWOBJECT_FOR_MC > 0) 
                     tmBuilder.addDelete(ProtobufUtil.toMutation(MutationType.DELETE, wa.getDelete()));
				   else
                     tmBuilder.addDelete(ProtobufUtil.toMutation(MutationType.DELETE, new Delete(wa.getDelete())));
                   iDelete++;
                }
             }
             msgCounter++;
             msgSize += wa.getCellsMemSize();
          } // for WriteAction loop

          tmBuilder.setIsMsgComplete(true);
          splitSeq++;
          tmBuilder.setSplitSeq(splitSeq);
          // now append the mutation into the circular MC queue, then return
          // the actual append and copy will be performed by the chore thread 

          synchronized(mutationBufferLock) {          
              bufferSeqId = circularBuffer_Attach(tmBuilder, iPut, iDelete, ts.getMutationClient());
          }
          //18371
          //move the binlog append outside of mutationBufferLock lock area
          if (ATRConfig.instance().isATRXDCEnabled() && ( binlogIsMaxProtectionMode() == false || nonTrxRecordBinlog == true) )
          {
              currBinlogWid = binlog_append(tmBuilder, ATRConfig.instance().getSyncMode());
          }

          if (LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- txnMutationAppend -- Transaction Id " + ts.getTransactionId() +
                              " commit id " + ts.getCommitId() + " Table CDC Attribute " + ts.getMutationClient() + 
                              " has " + iPut + " put " + iDelete + " delete for region " + this.regionInfo.getRegionNameAsString() + 
                               " mutation captured in MC buffer sequence " + bufferSeqId);
      
      } catch(IOException e) {
              LOG.error("PIT MC2 -- txnMutationAppend -- Transaction Id " + ts.getTransactionId() + " stack ", e);
              throw new IOException(e);
      }

  }
  
  // Transactional Mutation Capturer
   
  public void MC2_doBufferAppend(long tid, long cid, TransactionMutationMsg.Builder tmBuilder, int iPut, int iDelete, int mClient, boolean doBinlog) throws IOException {
      
      int bufferSeqId = -1;
  
      if(LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- txnBufferAppend -- Transaction Id " + tid + 
                                                          " commitId " + cid + " MC2_attribute " + MC2_attribute + " skip " + skip_CDC);

      if (skip_CDC) return;

      if (pSTRConfig.getPeerStatus(pSTRConfig.getTrafClusterIdInt(), true).contains(PeerInfo.PEER_ATTRIBUTE_NO_CDC_IBR)) {
         if (LOG.isDebugEnabled()) LOG.debug("PIT MC2: buffer append: STRCONFIG NO_CDC_IBR is true, trans id " +  tid + 
                        " CDC attribute " + mClient);
         boolean ibr_mc = ((mClient & INCREMENTALBR) == INCREMENTALBR);
         boolean xdc_catchup =  (((mClient & SYNCHRONIZED) == SYNCHRONIZED)
                                           && ((mClient & XDC_DOWN) == XDC_DOWN)  );
                                           
         if (  (ibr_mc) && (!xdc_catchup)  ) {
            if (LOG.isDebugEnabled()) LOG.debug("PIT MC2: buffer append: bypass IBR CDC due to STRCONFIG NO_CDC_IBR, trans id " + tid + 
                     " CDC attribute " + mClient);
            return;
         }
      }      

      try {

          // now append the mutation into the circular MC queue, then return
          // the actual append and copy will be performed by the chore thread 

          synchronized(mutationBufferLock) {          
              bufferSeqId = circularBuffer_Attach(tmBuilder, iPut, iDelete, mClient);
          }
          //18371
          //move the binlog append outside of mutationBufferLock lock area
          if (ATRConfig.instance().isATRXDCEnabled()  && binlogIsMaxProtectionMode() == false && doBinlog == true)
          {
              currBinlogWid = binlog_append_buffer(tmBuilder, ATRConfig.instance().getSyncMode());
          }

          if (LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- txnBufferAppend -- Transaction Id " + tid +
                              " commit id " + cid + " Table CDC Attribute " + mClient + 
                              " has " + iPut + " put " + iDelete + " delete for region " + this.regionInfo.getRegionNameAsString() + 
                               " mutation captured in MC buffer sequence " + bufferSeqId);
      
      } catch(IOException e) {
              LOG.error("PIT MC2 -- txnMutationAppend -- Transaction Id " + tid + " stack ", e);
              throw new IOException(e);
      }

  }
  

  public void MC2_doWriterOperation(int action) throws IOException {
   
  // this is used by any request whioch want to do writer operation ionline, the caller will wait until completion  

      int bTxId = -1;
      int wTxId = -1;
      int cFront = -1;
      int cRear = -1;
      int cCount = 0;
	  boolean doClose = false;
      
      if(LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- Writer Action: " + action);

      synchronized(mutationWriterLock) {
      
          try {
      
          switch (action) {
              case PIT2_MUTATION_WRITER_CREATE: { // create next transactional mutation file and associated HBaseWriter
                  // create a writer if not yet
                  if (mutationWriter == null) {
                      doHFileCreate();
                  }
                  break;
              }
             case PIT2_MUTATION_WRITER_METAUPDATE: { 
                  if (mutationWriter != null) {
                      doMetaUpdate();
                  }
                  break;
              }
              case PIT2_MUTATION_WRITER_APPEND:
              case PIT2_MUTATION_WRITER_CLOSE: { // append the mutation into output buffer with delimeter

                  synchronized(mutationBufferLock) {  // grab the range to append
                      cCount = qCount;
                  }
                  if (LOG.isDebugEnabled())
                      LOG.debug("PIT MC2 -- Writer Append: AAA1 " + qCount);
                  if (cCount > 0) { // only has mutations to be appended

                     // create a writer if not yet
                     if (mutationWriter == null) {
                         doHFileCreate();
                     }

                     //bTxId = circularBuffer_Remove(cFront, cRear, cCount);
                     synchronized(mutationBufferLock) {  // release range
                         cFront = qFront;
                         cRear = qRear;
                         cCount = qCount;
                         bTxId = circularBuffer_Remove(cFront, cRear, cCount);		       
                         if (LOG.isDebugEnabled())
                             LOG.debug("PIT MC2 -- Writer Append: AAA2 remove element " + cCount + " iput " + totalPuts + " idelete " + totalDeletes);
                         qFront = (cFront + cCount) % qSize;
                         qCount = qCount - cCount;
                         //COMMIT_MEMORY_CHECK_LOG_MARK is 0x20
                         if (LOG.isDebugEnabled() || (RSConstants.PRINT_TRANSACTION_LOG & 0x20) > 0)
                              printMemoryUsage("MC2_doWriterOperation before doHFileAppend() mutationBuffer length " + mutationBuffer.size());

                         doHFileAppend();

                         if (LOG.isDebugEnabled() || (RSConstants.PRINT_TRANSACTION_LOG & 0x20) > 0)
                             printMemoryUsage("MC2_doWriterOperation after doHFileAppend()");

                         if (LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- circular buffer for region " + 
                            this.regionInfo.getRegionNameAsString() + 
                            " C queue info " + qFront + " , " + qRear + " size " + qCount + " capacity " + qSize);                           
                     }
                     // ensure close this mutation file right now          
                     doClose = do_closeCheck();

                     if ((action == PIT2_MUTATION_WRITER_CLOSE) || doClose) {
                        doHFileClose();
                     }
                  }
                  if ((mutationCount % 50) == 49) { // print every 50 times
                     if(LOG.isInfoEnabled()) LOG.info("PIT MC2 -- mutationWriterAction: append " + action + " " +
                         regionInfo.getTable().toString() + "-e-" + regionInfo.getEncodedName() + 
                         " FileKey " + currentFileKey +
                         " PIT mutation file path " + currentMC2Path +
                         " mutationCount " + mutationCount + " mutationSet " + mutationSet +
                         " smallest CommitId " + smallestCommitId + 
                         " largest CommitId " + largestCommitId +                         
                         " total puts " + totalPuts + " total deletes " + totalDeletes);
                     if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- circular buffer for region " + 
                         this.regionInfo.getRegionNameAsString() + 
                         " C queue info " + qFront + " , " + qRear + " size " + qCount + " capacity " + qSize);                      
                  }
                  
                  if (cCount <= 0) { // close this mutation file even though no data to be writen
                    doClose = do_closeCheck();

                    if ((action == PIT2_MUTATION_WRITER_CLOSE) || doClose) {
                        doHFileClose();
                    }
                  }
                  break;
              } // case
             default: { // bad parameter
                  if(LOG.isWarnEnabled()) LOG.warn("PIT MC2 -- mutationWriterAction: invalid action " + action);
              }
          } // switch     
          } catch(Exception e) {
                  LOG.error("PIT MC2 -- CDC Exception: Writer Operation " + action + " stack ", e);
                  if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- circular buffer for region " +
                      this.regionInfo.getRegionNameAsString() + 
                      " C queue info " + qFront + " , " + qRear + " size " + qCount + " capacity " + qSize);                   
                  throw new IOException(e);
          } // catch block

      } // mutationWriterLock
  }
  
  boolean do_closeCheck() {

      if ((mutationTotalCount >= this.PIT_MC2_max_mutation_perFILE) || 
                       (mutationTotalSize >= this.PIT_MC2_max_size_peFILE)) { 
           if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- writer check if over mutation file close threshold " +
                                        regionInfo.getTable().toString() + "-e-" + regionInfo.getEncodedName() + 
                                        " FileKey " + currentFileKey +
                                        " PIT mutation file path " + currentMC2Path +
                                        " with smallest commitId " + smallestCommitId +
                                        " with largest commitId " + largestCommitId +                                           
                                        " number of total Txn Protos " + mutationTotalCount + 
                                        " total txn mutation size " + mutationTotalSize + 
                                        " total puts " + totalPuts + " total deletes " + totalDeletes);
           return true;
      }
      return false;
  } 

  public void setNonTrxRecordBinlog(boolean v) { nonTrxRecordBinlog = v; }

  public void MC2_doChoreOperation() {

      int cFront = -1;
      int cRear = -1;
      int cCount = 0;
      int bTxId = -1;
      int wTxId = -1;

      // MC2 chore will run this procedure based on chore timer, and examine the need to do any writer operation
      // every write operation will get a monotonic txid
      // -- writer create, close
      // -- writer MF append
      // -- streamer append, flush

      try { 

          if ((choreTxId.get() % 100) == 1) {
              if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- Chore Operation Seq " + choreTxId.getAndIncrement() +
                               " for region " + this.regionInfo.getRegionNameAsString());
 
              if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- Writer Operation Seq " + writerTxId.getAndIncrement() +
                               " for region " + this.regionInfo.getRegionNameAsString());  
          }

          // do file append if need (doAppend can be set by Chore/Txn Thread if certain threshold is reached
          // do file close if need
          //      a) total mutation size is too large -- chore 
          //      b) system event such as pre-flush by HBase (memory store is going to flush) -- API
          //      c) per request (e.g. during snapshot or recovery request) -- API
          //      d) number of total MC or timer through lease -- chore

          MC2_doWriterOperation(PIT2_MUTATION_WRITER_APPEND);
          // MC2_doWriterOperation(PIT_MUTATION_WRITER_CLOSE);
      
      } catch(IOException e) {
          LOG.error("PIT MC2 -- CDC Exception: do Chore Operation stack ", e);
          //
          // what to do here, defer, or kill RS if pass threshold
          // throw new IOException(e);
      } // catch block

  }

  private int circularBuffer_Attach(TransactionMutationMsg.Builder tmBuilder, int iput, int idelete, int mClient) throws IOException {

  // caller must acquire buffer lock
  // link the tmBuilder into next slot in the CDC queue, and grad the txid (monotonic append sequence) back
          
          if (LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- Buffer Operation Seq " + bufferTxId.get() +
                  " circular buffer produce for region " + this.regionInfo.getRegionNameAsString());        

          try { 
              if (qCount == qSize) { // MC queue full
                  // instead of increase the queue, return a -2 RETRY, so the caller would retry later especting the
                  // chore thread will flush and cleanup the queue
                  set_CircularQueueCapacity(qSize*2, false);
              }

              mcQueue[qRear] = tmBuilder;
              iPutStat[qRear] = iput;
              iDeleteStat[qRear] =idelete;
              qRear = (qRear + 1) % qSize;
              qCount++;
              if (LOG.isDebugEnabled())
                  LOG.debug("PIT MC2 -- attach mutation " + qRear + " size " + qCount);

              if (LOG.isDebugEnabled()) LOG.debug("HAX - CB attach -- Transaction Id " + tmBuilder.getTxId() +
                              " commit id " + tmBuilder.getCommitId() +
                              " Table CDC Attribute " + tmBuilder.getTableCDCAttr() + 
                              " has " + iput + " put, " + idelete + " delete for region " + this.regionInfo.getRegionNameAsString() + 
                              " mutation added into CB " + bufferTxId.get());  

          } catch (Exception exc) {
              LOG.error("PIT MC2 -- CDC Exception: exception during mutation append to buffer ", exc);
              throw new IOException(exc);
          } finally {
              return bufferTxId.getAndIncrement();
          }
  }

  private int circularBuffer_Remove(int cFront, int cRear, int cCount) throws IOException {

  // caller must setup the range to be blocked by otheer consumer or any producer
  // build byte array data stream from the the range (fron-rear) for append

      int iFront = cFront;
      int iCount = cCount;

          if (LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- Buffer Operation Seq " + bufferTxId.get() +
                  " circular buffer consume for region " + this.regionInfo.getRegionNameAsString() + 
                  " C queue range " + cFront + " , " + cRear + " size " + cCount + 
                  " C queue info " + qFront + " , " + qRear + " size " + qCount + " capacity " + qSize);        
      
          try {
              if ((mutationBuffer != null) && (mutationBuffer.size() > 0)) {
                  LOG.error("PIT MC2 -- Buffer Operation Seq " + bufferTxId.get() +
                        " circular buffer consume for region " + this.regionInfo.getRegionNameAsString() +
                        " C queue range " + cFront + " , " + cRear + " size " + cCount +
                        " C queue info " + qFront + " , " + qRear + " size " + qCount + " capacity " + qSize +
                        " mutationBuffer size is :" + mutationBuffer.size());
              }
              mutationBuffer = new ByteArrayOutputStream();
              while (iCount > 0) { // the range of mutations in queue to be collected is not empty
                  TransactionMutationMsg.Builder builder = mcQueue[iFront];
                  // sanity check if builder == null
                  totalPuts = totalPuts + iPutStat[iFront];
                  totalDeletes = totalDeletes + iDeleteStat[iFront];
                  if (builder != null) {
                      builder.build().writeDelimitedTo(mutationBuffer);
                      if ((smallestCommitId == -1) || (smallestCommitId > builder.getCommitId()))
                                     smallestCommitId = builder.getCommitId();  
                      if ((largestCommitId == -1) || (largestCommitId < builder.getCommitId()))
                                     largestCommitId = builder.getCommitId();	// use Long.MAX_VALUE
                  }
                  else {
                      LOG.warn("PIT MC2 -- CDC Exception: builder null exception during mutation remove from buffer");                      
                  }
                  // reset slot in qeue
                  mcQueue[iFront] = null;
                  iPutStat[iFront] = 0;
                  iDeleteStat[iFront] = 0;
                  // advance to next Slot
                  iFront = (iFront + 1) % qSize;
                  iCount--;
                  // update statistics
                  mutationCount++;
                  mutationTotalCount++;
              }
          }
          catch(NullPointerException npe){ 
              LOG.error("PIT MC2 -- CDC Exception: ignored, null pointer exception during mutation remove from buffer  ", npe);              
          }
          catch (Exception exc) {
              LOG.error("PIT MC2 -- CDC Exception: exception during mutation remove from buffer  ", exc);
          }

          return bufferTxId.getAndIncrement();
  }

  public void MC2_expandCircularQueueCapacity(int newSize) throws IOException {

      synchronized(mutationBufferLock) {
          try { 
              set_CircularQueueCapacity(newSize, false);
          } catch (Exception exc) {
              throw new IOException(exc);
          }
      }

  }

  private void set_CircularQueueCapacity(int newSize, boolean init) throws IOException {

  // caller supposed to have buffer lock already
  
      TransactionMutationMsg.Builder [] largerQueue;
      int [] largerPutStat;
      int [] largerDeleteStat;

          if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- Buffer Operation Seq " + bufferTxId.get() +
                  " buffer queue expand for region " + this.regionInfo.getRegionNameAsString() + " initialization " + init);

          try { 
              if (newSize <= qSize) newSize = qSize * 2;        

              largerQueue = new TransactionMutationMsg.Builder[newSize];
              largerPutStat = new int[newSize];
              largerDeleteStat = new int[newSize];

              if (init == false) { // only copy if needed
                  for(int scan=0; scan < qCount; scan++) {
                      largerQueue[scan] = mcQueue[qFront];
                      iPutStat[scan] = largerPutStat[qFront];
                      iDeleteStat[scan] = largerDeleteStat[qFront];
                      qFront=(qFront+1) % qSize;
                  }
              }

              if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- Buffer Operation Seq " + bufferTxId.get() +
                  " buffer queue expand for region " + this.regionInfo.getRegionNameAsString() + " to size "  + newSize + " from " + qSize);

              qFront = 0;
              qRear = qCount;
              qSize = newSize;
              mcQueue = largerQueue;
              iPutStat = largerPutStat;
              iDeleteStat = largerDeleteStat;

          } catch (Exception exc) {
              LOG.error("PIT MC2 -- CDC Exception: exception during mutation queue expand  ", exc);
              throw new IOException(exc);
          }

  }

  public void doHFileAppend() throws IOException {

      // caller must acquire writer lock 
      // append the byte array to HBase writer (later, could be HDFS file like sequence file)
      
      byte [] bKey;      

          if(LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- Writer Operation Seq " + writerTxId.getAndIncrement() +
              " HFile append for region " + this.regionInfo.getRegionNameAsString());        

          try {               
             mutationSet++;
             bKey = Bytes.toBytes (mutationSet);
             //bKey = concat(Bytes.toBytes(COMMITTED_TXNS_KEY), bSet);
             byte [] firstByte = mutationBuffer.toByteArray();

             mutationWriter.append(new KeyValue(bKey, Bytes.toBytes("cf"), Bytes.toBytes("qual"), firstByte));
/*SST
			 LongWritable sKey = new LongWritable(mutationSet);
			 BytesWritable sValue = new BytesWritable(firstByte);
			 mutationWriter.append(sKey, sValue);
			 mutationWriter.hsync();
*/
             if (LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- mutationWriterAction: action flush " +
                    regionInfo.getTable().toString() + "-e-" + regionInfo.getEncodedName() +
                    " mutationCount " + mutationCount + " mutationSet " + mutationSet + 
                    " all mutations in KV size " + firstByte.length + " mutationTotalCount " + mutationTotalCount);

             mutationTotalSize = mutationTotalSize + firstByte.length;
             mutationBuffer = null; // free the mutation bytearray context
             mutationCount = 0;

			//SST verify_Reader(); // muttaionWriter, MF reading validation

             if (LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- doFileAppend: file append "
                     + regionInfo.getTable().toString() + "-e-" + regionInfo.getEncodedName() +
                      " FileKey " + currentFileKey);

          } catch (Exception exc) {
                  LOG.error("PIT MC2 -- CDC Exception: exception during mutation file append ", exc);
                  throw new IOException(exc);
          }
  }
  
  public void verify_Reader() throws IOException{
	  
	  SequenceFile.Reader reader = null;
	  
	  try {
		 if (LOG.isInfoEnabled())
		    LOG.info("BBB reader path " + currentMC2Path);
	     reader = new SequenceFile.Reader(fs, currentMC2Path, config);
	     LongWritable key = new LongWritable();
         BytesWritable value = new BytesWritable();
		 
		 int i = 0;
         while (reader.next(key, value)) {
                long index = key.get();
                byte[] byteArray = value.getBytes();
                if (LOG.isInfoEnabled())
                    LOG.info("BBB --- " + i + " key " + index + " value " + byteArray.toString());
          }
          
          DFSInputStream open =  new DFSClient(config).open(currentMC2Path.toUri().getPath());
		  long fileLength = open.getFileLength();
		  long length = fs.getLength(currentMC2Path);
		  if (LOG.isInfoEnabled())
		      LOG.info("BBB ^^^ " + fileLength + " len " + length);
        }
        catch (IOException iop) {
			 LOG.error("BBB zzz ", iop);
			 throw new IOException(iop);
		}
		reader.close();
  }

  public void doHFileCreate() throws IOException {

      // caller must acquire writer lock
      // create file only when there are mutations queued
      // this operation is usually exercised through chore thread when it detects there is a need to create MF

      // TBD -- when a table is created or altered with IBR attribute, create a snapshot servig the base (non-user defined tag)
      
      MutationMetaRecord mmr;
      boolean test = false;      
 
          if (LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- Writer Operation Seq " + writerTxId.getAndIncrement() +
              " create mutation file for region " + this.regionInfo.getRegionNameAsString());        

          if (snapMeta == null) {
              try {
                  snapMeta = new SnapshotMeta(this.config);
              } catch (Exception e){
                  LOG.error("PIT MC2 -- CDC Exception: creating new snapshot meta: ", e);
                  throw new IOException(e);
              }
          }

          if (meta == null) {
              try {
                  meta = new MutationMeta(this.config);
              } catch (Exception e){
                  LOG.error("PIT MC2 -- CDC Exception: creating new mutation meta: ", e);
                  throw new IOException(e);
              }
          }
              
          try {
              currentSnapshot = snapMeta.getCurrentSnapshotRecord(regionInfo.getTable().toString());
              if ((currentSnapshot == null) || currentSnapshot.isSkipTag()) {
                  currentSnapshotId = 999; // Note. use 999 as holder for all the cases (even w/o initial snapshot from the beginning)
              }
              else {
                  currentSnapshotId = currentSnapshot.getKey();
               }
                         
              if(LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- get current snapshot id " + currentSnapshotId + " table " + regionInfo.getTable().toString());
          } catch (Exception e){
              LOG.error("PIT MC2 -- CDC Exception: get current snapshot id failed " + regionInfo.getTable().toString() + " Exception: ", e);
              currentSnapshotId = 999; //special snapshotId
          }

          try {
              SnapshotMetaIncrementalRecord incRec = snapMeta.getPriorIncrementalSnapshotRecord(
                                 regionInfo.getTable().toString(), /* includeExcluded */ true);
              if (incRec != null){
                  incrementalSnapshotId = incRec.getKey();
                  if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- get incremental snapshot id "
                          + incrementalSnapshotId + " " + regionInfo.getTable().toString());
              }
              else {
                  incrementalSnapshotId = -1;
                  if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- setting incrementalSnapshotId to -1 because "
                          + "no prior incremental snapshot found for " + regionInfo.getTable().toString());
               }
          } catch (Exception e){
              LOG.error("PIT MC2 -- CDC Exception: get incremental snapshot id failed " + regionInfo.getTable().toString() + " Exception: ", e);
              incrementalSnapshotId = -1; // None
          }

          // snapshot name cannot contain ":". If the src name is prefixed by "namespace:",
          // replace ":" with "_"
          String pathName = regionInfo.getTable().toString().replace(":", "_");

          boolean done = false; // register the unique mmr record in mutation meta hbase table
          while (! done) {
              long oldKey = EnvironmentEdgeManager.currentTime();
              currentFileKey = regionInfo.getTable().toString() + "_" + oldKey;
              if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- intend to create mutation meta file ... ");
              currentMC2Path = new Path(PITRoot +
                      pathName + "-snapshot-" + Long.toString(currentSnapshotId) +
                      "-e-" + regionInfo.getEncodedName() +
                      "-" + oldKey);
              if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- Creating mutation meta file using write Path " + currentMC2Path);

              try {
                  mmr = new MutationMetaRecord(currentFileKey, regionInfo.getTable().toString(),
                   currentSnapshotId, incrementalSnapshotId,
                   -1 /* backupSnapshot */,
                   -1 /* supersedingullSnapshot */,
                   0 /* smallestCommitId */,
                   0 /* largestCommitId */,
                   512 /* fileSize */,
                  "PENDING" /* userTag */,
                  regionInfo.getEncodedName(), // region encoded Name as String
                  currentMC2Path.toString(), // mutation file path as String
                  true /* inLocalFS */,
                  false /* archived */,
                  "test" /* archive location */);
                  if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- CREATE mutation meta record created successfully for key " + currentFileKey);
                  if (meta ==  null){
                     LOG.error ("PIT MC2 -- mutation meta is null");
                  }
                  String retVal = meta.putMutationRecord(mmr, /* blindWrite */ false);
                  if (!currentFileKey.equals(retVal)) {
                     if(LOG.isInfoEnabled()) LOG.info("PIT MC2 -- Mutation putMutationRecord returned " + retVal
                               + "  Recreating MutationMetaRecord and retrying");
                  }
                  else {
                      done = true;
                      if(LOG.isInfoEnabled()) LOG.info("PIT MC2 -- Mutation Record Put Create : key " + mmr.getRowKey() +
                                " snapshot id " + currentSnapshotId +
                                " incremental snapshot id " + incrementalSnapshotId +
                                " table " + regionInfo.getTable().toString() +
                                " region encoded name " + regionInfo.getRegionNameAsString() +
                                " path " + currentMC2Path.toString() +
                                " inLocalFS: " + true +
                                " archived: " + false + " archive location: test" );
                  }

              } catch (Exception exc) {
                  LOG.error("PIT MC2 -- CDC Exception: put mutation record exception during mutation file creation ", exc);
                  throw new IOException(exc);
              }

          } // while (! done)      
                                           
          // create writer
          if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- create Mutation Writer with path " + currentMC2Path);
  
          mutationWriter = (HFileWriterV2) HFile.getWriterFactory(config,
                        new CacheConfig(config)).withPath(fs, currentMC2Path).withFileContext(context).create();
		  //SST mutationWriter = SequenceFile.createWriter(fs, config, currentMC2Path, LongWritable.class, BytesWritable.class);

          mutationCount = 0;
          mutationTotalCount = 0;
          mutationTotalSize = 0;
          totalPuts = totalDeletes = 0;
          smallestCommitId = -1;
          largestCommitId = -1;	  
          mutationSet = 0;
          
          if(LOG.isInfoEnabled()) LOG.info("PIT MC2 -- mutation writer create completes" +
                       " Table " + regionInfo.getTable().toString() + "-e-" + regionInfo.getEncodedName() +
                       " FileKey " + currentFileKey + " Path " + currentMC2Path);

  }

  public void doHFileClose() throws IOException {
 
      // caller must acquire writer lpock, close the writer and update meta
      
      MutationMetaRecord mmr = null;
      boolean noPit2 = false;   

      try{
          if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- Writer Operation Seq " + writerTxId.getAndIncrement() +
                      " mutation file close for region " + this.regionInfo.getRegionNameAsString() + " currentFileKety " + 
                      currentFileKey + " MC path " + currentMC2Path + " writer " + mutationWriter);     

          if ((mutationWriter != null) && ("".equals(currentFileKey))) {
                    LOG.error("PIT MC2 -- CDC Exception: ZZZ 1 mutation writer close  " + regionInfo.getTable().toString());
          }
          else if ((mutationWriter == null) && (!"".equals(currentFileKey))) {
                    LOG.error("PIT MC2 -- CDC Exception: ZZZ 2 mutation writer close  " + regionInfo.getTable().toString());
          }
        
          if ((mutationWriter != null) && (!"".equals(currentFileKey))) {
              try {
                  mutationWriter.close(); 
              } catch(IOException e) {
                  LOG.error("PIT MC2 -- CDC Exception: ZZZ 3 mutation writer close  " + regionInfo.getTable().toString());
                  noPit2 = true;
                  throw e;
              }
              mutationWriter = null; 

              if (currentSnapshotId == 999) { // TBD -- special case, update meta if possible
                  if (LOG.isInfoEnabled())
                      LOG.info("PIT MC2 -- BBB 3: mutation writer close  snapshot 999 " + regionInfo.getTable().toString());
                  try {
                      currentSnapshot = snapMeta.getCurrentSnapshotRecord(regionInfo.getTable().toString());
                      if ((currentSnapshot == null) || currentSnapshot.isSkipTag()) {
                          if (LOG.isInfoEnabled())
                              LOG.info("PIT MC2 -- current snapshot record not found during PIT Mutation Writer Close; setting sid == 999 "
                                     + regionInfo.getTable().toString());
                          currentSnapshotId = 999;
                      }
                      else {
                          currentSnapshotId = currentSnapshot.getKey();
                          if (LOG.isInfoEnabled()) LOG.info("PIT MC2 -- get current snapshot id " + currentSnapshotId +
                                                               " Table " + regionInfo.getTable().toString());  
                       }
                  } catch (Exception e){
                      LOG.error("PIT MC2 -- CDC Exception: get current snapshot id failed during PIT Mutation Writer Close when sid == 999 "
                                                     + regionInfo.getTable().toString() + " Exception: ", e);
                      currentSnapshotId = 999;
                  }
              }

              // update meta after writer close
              if (LOG.isInfoEnabled())
                  LOG.info("PIT MC2 -- CLOSE updating MMR for current snapshot record " + currentFileKey + " "
                                     + regionInfo.getTable().toString());
         
              try {
                  mmr = new MutationMetaRecord(currentFileKey, regionInfo.getTable().toString(), 
				    currentSnapshotId, incrementalSnapshotId, -1 /* backupSnapshot */,
				    -1 /* supersedingullSnapshot */,
				    smallestCommitId, largestCommitId, mutationTotalSize, // for commitId range & fileSize
				    "PENDING", // userTag updated by BackupRestoreClient during backup operation
				    regionInfo.getEncodedName(), // region encoded Name as String
				    currentMC2Path.toString(), // mutation file path as String
				    true /* inLocalFS */,
				    false /* archived */, "test" /* archive location */);

                  if(LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- CLOSE mutation meta record created successfully for key " + currentFileKey);
                  meta.putMutationRecord(mmr, /* blindWrite */ true);
                  if(LOG.isInfoEnabled()) LOG.info("PIT MC2 -- Mutation Record Put Close : record " + mmr );

                  } catch (Exception exc) {
                      LOG.error("PIT MC2 -- CDC Exception: put mutation record exception during mutation file close ", exc);
                      if(LOG.isInfoEnabled()) LOG.info("PIT MC2 -- Mutation Record Put Close : record " + mmr );
                  }

                  if(LOG.isInfoEnabled()) LOG.info("PIT MC2 -- writer close completes " +
                                        regionInfo.getTable().toString() + "-e-" + regionInfo.getEncodedName() + 
                                        " FileKey " + currentFileKey +
                                        " PIT mutation file path " + currentMC2Path +
                                        " with smallest commitId " + smallestCommitId +
                                        " with largest commitId " + largestCommitId +                                             
                                        " number of total Txn Protos " + mutationTotalCount + 
                                        " total txn mutation size " + mutationTotalSize + 
                                        " total puts " + totalPuts + " total deletes " + totalDeletes);

              } // writer is not null

     } catch(Exception e) {
        if (noPit2) LOG.error("PIT MC2 CDC Exception: path: unassociated pending mc file deleted forcefully ");
        else {
           LOG.error("PITMC2 CDC Exception: doHFileClose, stack ", e);
        }
     } finally {
        if(LOG.isInfoEnabled()) LOG.info("PIT MC2 -- writer close reset counters for region " + regionInfo.getRegionNameAsString());
        currentFileKey = "";
        mutationWriter = null;
        currentMC2Path = null;

        mutationCount = 0;
        mutationTotalCount = 0;
        mutationTotalSize = 0;
        totalPuts = totalDeletes = 0;
        smallestCommitId = -1;
        largestCommitId = -1;
        mutationSet = 0;

     }

  }

  public void doMetaUpdate() throws IOException {

      // caller must acquire the writer lock, then update meta
      
      MutationMetaRecord mmr;
      boolean test = false;      

          if(LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- Writer Operation Seq " + writerTxId.getAndIncrement() +
              " meta update for region " + this.regionInfo.getRegionNameAsString());     
         
              try {
                  mmr = new MutationMetaRecord(currentFileKey, regionInfo.getTable().toString(), 
					currentSnapshotId, incrementalSnapshotId, -1 /* backupSnapshot */,
					-1 /* supersedingullSnapshot */,
					smallestCommitId, largestCommitId, mutationTotalSize, // for commitId range & fileSize
					"PENDING", // userTag updated by BackupRestoreClient during backup operation
					regionInfo.getEncodedName(), // region encoded Name as String
					currentMC2Path.toString(), // mutation file path as String
					true /* inLocalFS */,
					false /* archived */, "test" /* archive location */);
                  if(LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- UPDATE mutation meta record created successfully for key " + currentFileKey);
                  meta.putMutationRecord(mmr, /* blindWrite */ true);
                  if(LOG.isDebugEnabled()) LOG.debug("PIT MC2 -- Mutation Meta Record Put : key " + currentFileKey +
				" snapshot id " + currentSnapshotId +
				" incremental snapshot id " + incrementalSnapshotId +
				" table " + regionInfo.getTable().toString() + 
				" region encoded name " + regionInfo.getRegionNameAsString() +
				" path " + currentMC2Path +
				" inLocalFS: " + true +
				" archived: " + false + " archive location: test" );                   
              } catch (Exception exc) {
                  LOG.error("PIT MC2 -- CDC Exception: put mutation record exception during mutation file close ", exc);
                  throw new IOException(exc);
              }

              if(LOG.isInfoEnabled()) LOG.info("PIT MC2 -- mutation meta update " +
                                        regionInfo.getTable().toString() + "-e-" + regionInfo.getEncodedName() + 
                                        " FileKey " + currentFileKey +
                                        " PIT mutation file path " + currentMC2Path +
                                        " with smallest commitId " + smallestCommitId +
                                        " with largest commitId " + largestCommitId +                                        
                                        " number of total Txn Protos " + mutationTotalCount + 
                                        " total txn mutation size " + mutationTotalSize + 
                                        " total puts " + totalPuts + " total deletes " + totalDeletes);

  }


  public void doStreamAppend() {


  }

  
  public void addPut(TransactionMutationMsg.Builder tmBuilder, Put put) throws IOException {
      try {
	  	  if (RSConstants.DISABLE_NEWOBJECT_FOR_MC > 0) 
          	 tmBuilder.addPut(ProtobufUtil.toMutation(MutationType.PUT, put));
		  else
		  	 tmBuilder.addPut(ProtobufUtil.toMutation(MutationType.PUT, new Put(put)));
      } catch(IOException e) {
          StringWriter sw = new StringWriter();
          PrintWriter pw = new PrintWriter(sw);
          e.printStackTrace(pw);
          LOG.error("PIT MC2 -- addPut " + sw.toString());
      }
  }
  
  public void addDelete(TransactionMutationMsg.Builder tmBuilder, Delete del) throws IOException {
      try {
	  	  if (RSConstants.DISABLE_NEWOBJECT_FOR_MC > 0) 
            tmBuilder.addDelete(ProtobufUtil.toMutation(MutationType.DELETE, del));
		  else
			tmBuilder.addDelete(ProtobufUtil.toMutation(MutationType.DELETE, new Delete(del)));
      } catch(IOException e) {
          StringWriter sw = new StringWriter();
          PrintWriter pw = new PrintWriter(sw);
          e.printStackTrace(pw);
          LOG.error("PIT MC2 -- addDelete " + sw.toString());
      }
  }

  // concatenate several byte[]
  byte[] concat(byte[]...arrays) {
       // Determine the length of the result byte array
       int totalLength = 0;
       for (int i = 0; i < arrays.length; i++)  {
           totalLength += arrays[i].length;
       }

       // create the result array
       byte[] result = new byte[totalLength];

       // copy the source arrays into the result array
       int currentIndex = 0;
       for (int i = 0; i < arrays.length; i++)  {
           System.arraycopy(arrays[i], 0, result, currentIndex, arrays[i].length);
           currentIndex += arrays[i].length;
       }
       return result;
  }

  public long getCurrBinlogWid() {
     if(currBinlogWid <= 0)
       currBinlogWid = HBaseBinlog.local_instance().getWriteIDnoInc();
     return currBinlogWid;
  }

  //append binlog into a file during recovery phase
  public long binlog_append_buffer(TransactionMutationMsg.Builder tmBuilder, String sync) throws IOException {
    return binlog_append(tmBuilder, sync);
  }

  public long binlog_append(TransactionMutationMsg.Builder tmBuilder, String sync) throws IOException {
      long writeID = 0L;
      if (ATRConfig.instance().isPrimaryCluster()) {
          boolean append_success = false;
          if (sync.equals(ATRConfig.SYNC_MODE_MAX_RELIABILITY) ||
              sync.equals(ATRConfig.SYNC_MODE_S_RELIABILITY) ) {
              sync = ATRConfig.SYNC_MODE_MAX_PERFORMANCE;
          }
          if (!ATRConfig.instance().isRemote()) {
            append_success = HBaseBinlog.local_instance().append(this.config, tmBuilder, regionInfo.getRegionNameAsString(), sync);
            writeID = HBaseBinlog.local_instance().getWriteIDnoInc();
          } else {
            if (ATRConfig.instance().isDualMode()) {
                synchronized(binlogDualAppendLock) {
                    writeID = HBaseBinlog.local_instance().getCurrentWriteID(this.config);
                    if(LOG.isDebugEnabled()) LOG.debug("MC2 binlog_append remote dual mode with WID:" + writeID);
                    boolean local_success = HBaseBinlog.local_instance().append(writeID, this.config, tmBuilder, regionInfo.getRegionNameAsString(), sync, false);
                    boolean remote_success = HBaseBinlog.remote_instance().append(writeID, this.config, tmBuilder, regionInfo.getRegionNameAsString(), sync, true);
                    append_success = (local_success && remote_success);
                }
            } else {
                append_success = HBaseBinlog.remote_instance().append(this.config, tmBuilder, regionInfo.getRegionNameAsString(), sync);
            }
          }
          if(ATRConfig.instance().getSyncMode().equals(ATRConfig.SYNC_MODE_MAX_PROTECTION) && !append_success) {
            throw new IOException("MAX_PROTECTION mode is configured.At lease one cluster binlog append failed.");
          } else {
            return writeID;
          }

      } else {
         LOG.warn("MutationCapture2 binlog appending ignored for not in primary cluster");
      }
      return -1L;
  }

  public boolean tableHasPrimaryKey(String tbl) throws IOException
  {
     String[] parts = tbl.split(","); //regionName is part 0
     if(!ATRConfig.instance().isATRXDCEnabled() || parts.length < 1 ) return true; // do not do keep rows by default
     return HBaseBinlog.local_instance().tableHasPrimaryKey(parts[0], this.config);
  }

  public int getBinlogSaltNum() {
    if(ATRConfig.instance().isATRXDCEnabled()) {
      return HBaseBinlog.local_instance().getMySalt(this.config);
    } else {
      return -1;
    }
  }

  public int isThisWidFlushed(long wid, long tid) {
    if (ATRConfig.instance().isDualMode())
    {
      // no need to check local status
      //int r1 = HBaseBinlog.local_instance().isThisWidFlushed(wid, tid) ;
      //if( r1 == HBaseBinlog.BINLOG_WRITE_ERROR) return HBaseBinlog.BINLOG_WRITE_ERROR;
      //else if( r1 == HBaseBinlog.BINLOG_FLUSH_WAIT) return HBaseBinlog.BINLOG_FLUSH_WAIT;
      //else {
        int r2 = HBaseBinlog.remote_instance().isThisWidFlushed(wid, tid) ;
        if( r2 == HBaseBinlog.BINLOG_WRITE_ERROR) return HBaseBinlog.BINLOG_WRITE_ERROR;
        else if( r2 == HBaseBinlog.BINLOG_FLUSH_WAIT) return HBaseBinlog.BINLOG_FLUSH_WAIT;
        else {
          return HBaseBinlog.BINLOG_FLUSH_OK; //all check OK
        }
      //}
    }
    else
      return HBaseBinlog.local_instance().isThisWidFlushed(wid, tid);
  }

  public boolean binlogIsMaxProtectionMode() {
    if (ATRConfig.instance().isATRXDCEnabled() && ATRConfig.instance().isMaxProtectionSyncMode())
        return true;
    else
       return false;
  }

  private void printMemoryUsage(String where) {
      if (memoryBean == null)
          memoryBean = ManagementFactory.getMemoryMXBean();
      long memUsed = memoryBean.getHeapMemoryUsage().getUsed();
      long memMax = memoryBean.getHeapMemoryUsage().getMax();
      if (memMax <= 0)
        return;
      int percent = (int)(memUsed * 1.0 / memMax * 100);
      LOG.info("[MEM] total: " + String.valueOf(memMax) + " inuse: " +
          String.valueOf(memUsed) + " in " + String.valueOf(percent) + " % on " + where);
  }

}

